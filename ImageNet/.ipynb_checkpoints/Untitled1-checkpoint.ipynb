{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from tensorflow.python.framework.ops import enable_eager_execution\n",
    "#disable_eager_execution()\n",
    "enable_eager_execution()\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_crop_shape(\n",
    "    image_height, image_width, aspect_ratio, crop_proportion):\n",
    "    \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
    "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
    "  less than or equal to `crop_proportion` along the other side.\n",
    "  Args:\n",
    "    image_height: Height of image to be cropped.\n",
    "    image_width: Width of image to be cropped.\n",
    "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    crop_height: Height of image after cropping.\n",
    "    crop_width: Width of image after cropping.\n",
    "  \"\"\"\n",
    "    image_width_float = tf.cast(image_width, tf.float32)\n",
    "    image_height_float = tf.cast(image_height, tf.float32)\n",
    "\n",
    "    def _requested_aspect_ratio_wider_than_image():\n",
    "        crop_height = tf.cast(tf.math.rint(\n",
    "            crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
    "        crop_width = tf.cast(tf.math.rint(\n",
    "            crop_proportion * image_width_float), tf.int32)\n",
    "        return crop_height, crop_width\n",
    "    \n",
    "    def _image_wider_than_requested_aspect_ratio():\n",
    "        crop_height = tf.cast(\n",
    "            tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
    "        crop_width = tf.cast(tf.math.rint(\n",
    "            crop_proportion * aspect_ratio *\n",
    "            image_height_float), tf.int32)\n",
    "        return crop_height, crop_width\n",
    "\n",
    "    return tf.cond(\n",
    "      aspect_ratio > image_width_float / image_height_float,\n",
    "      _requested_aspect_ratio_wider_than_image,\n",
    "      _image_wider_than_requested_aspect_ratio)\n",
    "\n",
    "\n",
    "def center_crop(image, height, width, crop_proportion):\n",
    "    \"\"\"Crops to center of image and rescales to desired size.\n",
    "  Args:\n",
    "    image: Image Tensor to crop.\n",
    "    height: Height of image to be cropped.\n",
    "    width: Width of image to be cropped.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
    "  \"\"\"\n",
    "    shape = tf.shape(image)\n",
    "    image_height = shape[0]\n",
    "    image_width = shape[1]\n",
    "    crop_height, crop_width = _compute_crop_shape(\n",
    "      image_height, image_width, height / width, crop_proportion)\n",
    "    offset_height = ((image_height - crop_height) + 1) // 2\n",
    "    offset_width = ((image_width - crop_width) + 1) // 2\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_height, offset_width, crop_height, crop_width)\n",
    "\n",
    "    image = tf.image.resize([image], [height, width])[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_image(features):\n",
    "    \"\"\"Preprocesses the given image.\n",
    "\n",
    "      Args:\n",
    "        image: `Tensor` representing an image of arbitrary size.\n",
    "\n",
    "      Returns:\n",
    "        A preprocessed image `Tensor` of range [0, 1].\n",
    "  \"\"\"\n",
    "    image = features[\"image\"]\n",
    "    image = tf.image.resize(image,[224,224])\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "    #image = center_crop(image, 224, 224, crop_proportion=0.875)   # Standard for ImageNet.\n",
    "    #image = tf.reshape(image, [224, 224, 3])\n",
    "    #image = tf.clip_by_value(image, 0., 1.)\n",
    "    \n",
    "    features[\"image\"] = image\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 50\n",
    "tfds_dataset, tfds_info  = tfds.load(name='imagenet2012_subset', split='validation', with_info=True,\n",
    "                                     data_dir='/local/rcs/wei/image_net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds_dataset.map(preprocess_image).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = tfds_info.splits['validation'].num_examples\n",
    "num_classes = tfds_info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.compat.v1.data.make_one_shot_iterator(tfds_dataset).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrees(y, y_hat, file_list, set_):\n",
    "    p = [i for i, j in enumerate(zip(list(y_hat.numpy()),list(y.numpy()))) if all(j[0]==k for k in j[1:])]\n",
    "    set_.update(file_list[i].numpy() for i in p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_ = set()\n",
    "def eval(model, ds, set_):\n",
    "    top_1_accuracy = tf.keras.metrics.Accuracy('top_1_accuracy')\n",
    "    for i, features in enumerate(ds):\n",
    "        logits = model(features[\"image\"])\n",
    "        top_1_accuracy.update_state(features[\"label\"], tf.argmax(logits, axis=-1))\n",
    "        agrees(features[\"label\"],tf.argmax(logits, axis=-1),features[\"file_name\"],set_)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(\"Finished %d examples\" % ((i + 1) * BATCH_SIZE))\n",
    "        if (i + 1) * BATCH_SIZE == 5000:\n",
    "            break\n",
    "    return top_1_accuracy.result().numpy().astype(float), features[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 224 ,224\n",
    "model = ResNet50(input_shape=(img_rows, img_cols,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2500 examples\n",
      "Finished 5000 examples\n",
      "Top-1: 71.2\n"
     ]
    }
   ],
   "source": [
    "results, images = eval(model,ds)\n",
    "print(\"Top-1: %.1f\" % (results * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3562"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "model2 = tfmot.quantization.keras.quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2500 examples\n",
      "Finished 5000 examples\n",
      "Top-1: 0.2\n"
     ]
    }
   ],
   "source": [
    "results, images = eval(model2,ds)\n",
    "print(\"Top-1: %.1f\" % (results * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3564"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
